\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}
\usepackage{listings}


\title{Building a Student Intervention System}

\author{Uir√° Caiado}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
% it is a comment. Pretty nice, hum 
There exists a push from educators and administrators to raise the likelihood of students successfully complete their programs. The aim of this project is to identify students who need intervention before they drop out of school. My goal is to use concepts from supervised machine learning to find the most effective model with the least amount of computation costs that identify such students.
\end{abstract}


\section{Introduction}
\label{sec:introduction}

\subsection{Some Background}
As stated by \href{https://www.udacity.com/course/viewer#!/c-nd009/l-5446988865/m-5446493941}{Udacity} in the description for this project, as education has grown to rely more and more on technology, more and more data is available for examination and prediction. Logs of student activities, grades, interactions with teachers and fellow students, and more are now captured through learning management systems.

Within all levels of education, there exists a surge to help increase the likelihood of student success without watering down the education or engaging in behaviors that raise the probability of passing metrics without improving the actual underlying learning. Graduation rates are often the criteria of choice for this, and educators and administrators are after new ways to predict success and failure early enough to stage effective interventions, as well as to identify the effectiveness of different interventions.

\subsection{The Goal}
The goal for this project is implementing a student intervention system using concepts from Supervised Machine Learning. I am going to choose and develop a model that will predict the likelihood that a given student will pass, thus helping diagnose whether or not an intervention is necessary.

I will suppose that the data available to create the model is a representative but a small sample of the dataset that I would have access in the production environment. So, besides the typical accuracy requirements for any machine-learning project, I am also going to look for models that are efficient in the use of resources (computation time and memory). Thus, the model will be evaluated on three factors:

\begin{itemize}
\item Its \href{https://en.wikipedia.org/wiki/F1_score}{$F_1$} Score, summarizing the number of correct positives and correct negatives out of all possible cases. In other words, how well does the model differentiate likely passes from failures
\item The size of the training set, preferring smaller training sets over larger ones. That is, how much data does the model need to make a reasonable prediction?
\item The computation resources to make a reliable prediction. How much time and memory is required to correctly identify students that need intervention?
\end{itemize}

\subsection{Classification vs Regression}
The model that will be developed is a Classifier. According to \cite{Hastie_2009}, the distinction in the output type has led to a naming convention for prediction tasks: \textit{regression} when we predict quantitative (continuous) outputs, and \textit{classification} when we predict qualitative outputs (discrete). As the goal of this project is to identify if the student will succeed, the problem posed is a classification problem once it requires a binary answer (passed, failed).


\section{Exploring the Data}
\label{sec:exploring_data}
In this section, we will explore the data to look for insides about the features.

\subsection{Basic Facts}
Let's go ahead and execute a basic description of the student dataset (Table \ref{tab:basicfacts}):

\begin{table}[ht]
\centering
\begin{tabular}{l|r}
Item & Value \\\hline
Total number of students & 42 \\
Total number of students & 395 \\
Number of students who passed & 265 \\
Number of students who failed & 130 \\
Number of features & 31 \\
Graduation rate of the class & 67.09 \%

\end{tabular}
\caption{\label{tab:basicfacts}Facts About the Dataset.}
\end{table}

All the 31 features in the dataset are discrete and Qualitative. There are 14 binary data, 13 ordered categorical variables and other 4 Categorical features. Among them, there are variables such as Father's Job, Family Size, if the student wants to take higher education or if he/she is in a romantic relationship. The data offers a pretty comprehensive profile of student life. At this point, it is hard to say what features are relevant or not. A complete description of each variable can be found on the Github \href{https://github.com/udacity/machine-learning/tree/master/projects/student_intervention}{project page}. There is no missing data on this dataset and no presence of ``real'' outliers.

\subsection{How the features are spread out}
Before moving on, I am going to plot each feature to see how each inner classes of each variable are divided between the target labels ``Passed'' and ``Not Passed''. In the Figure~\ref{fig:categorical} is plotted the Categorical features of the dataset. Here is possible to see that when the guardian is not the mother nor the father, it is more likely that the pupil not pass. Also, when the mother's occupatsion is teaching (\textit{Fjob}), is likely that the student to pass.

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{figures/categorical/categorical.png}
\caption{\label{fig:categorical}Categorical Data.}
\end{figure}

Looking at the Ordered Categorical Features (Figure~\ref{fig:ordCategorical}), where there is an ordering of the values, the features \textit{absences}, \textit{age} and \textit{failure} stand out. The \textit{failures} feature is the number of past class failures ($n$ if $1<=n<3$, else $4$) and absences is the number of school absences. In all these variables, the bigger the number, the higher the likelihood of the student does not pass.

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{figures/ordCategorical/ordCategorical.png}
\caption{\label{fig:ordCategorical}Ordered Data.}
\end{figure}

Finally, looking at the Binary Data (Figure~\ref{fig:binary}), what more draw the attention are the features \textit{higher} and \textit{schoolsup}. The feature \textit{higher} is if the student wants to take higher education and \textit{schoolsup} is if the student has extra educational support. Some features, as \textit{activities} and \textit{nursery}, presented a very similar distribution between the classes

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{figures/binary/binary.png}
\caption{\label{fig:binary}Binary Data.}
\end{figure}

As can be seen, there are a lot of features that might add up to the classification process, although some features may be not so relevant It is important to notice that the model developed with this amount of data (365 data points) and variables (31) can suffer from the  \href{https://en.wikipedia.org/wiki/Curse_of_dimensionality}{\textit{curse of dimensionality}}. Many of the features will be converted into dummy variables, what could increase the amount of data needed to build an accurate model. Maybe would be interesting to test if treat the binary data as numerical to reduce the necessity to split into dummies (reducing the dimension of the dataset) can increase the performance. It will be made in section 5.


\section{Preparing the Data}
In this section, I will prepare the data for modeling, training and testing.
\subsection{Identify feature and target columns}
It is often the case that the data you obtain contains non-numeric features. This can be a problem, as most machine learning algorithms expect numeric data to perform computations with.

% code snipet
\begin{lstlisting}
Feature column(s):
===================
school         	sex            	age            	address        
famsize        	Pstatus        	Medu           	Fedu           
Mjob           	Fjob           	reason         	guardian       
traveltime     	studytime      	failures       	schoolsup      
famsup         	paid           	activities     	nursery        
higher         	internet       	romantic       	famrel         
freetime       	goout          	Dalc           	Walc           
health         	absences       

Target column:
===================
passed

Feature values:
===================
  school sex  age address famsize Pstatus  Medu  Fedu
0     GP   F   18       U     GT3       A     4     4
1     GP   F   17       U     GT3       T     1     1
2     GP   F   15       U     LE3       T     1     1
3     GP   F   15       U     GT3       T     4     2
4     GP   F   16       U     GT3       T     3     3 ...
\end{lstlisting}

\subsection{Preprocess feature columns}
As can be seen above, there are several non-numeric columns in the dataset that need to be converted. The easiest case is when are only two categories, as ``yes'' or ``no'', e.g. \textit{internet}. These are often represented by single binary digit as $0$ or $1$.

According to \cite{Hastie_2009}, when there are more than two categories, the most commonly alternative used is coding via \href{https://www.moresteam.com/whitepapers/download/dummy-variables.pdf}{\textit{dummy variables}}. This method consists in representing a $K$-Level qualitative variable by a vector of $K$ binary variables, where only one of which is ``on'' at a time.

So, for example, the \textit{Fjob} feature will be split into $5$ new features ( \textit{Fjob\_teacher}, \textit{Fjob\_other}, \textit{Fjob\_services}, etc.), and will be assigned a $1$ to one of them and $0$ to all other columns.

This transformation will be performed using the pandas function `get\_dummies()`. Initially, all binary data that the classes do not correspond to $yes$/$no$ will be converted into dummies.

% insert code snippet
\begin{lstlisting}
Processed feature columns (48):
===================

school_GP         	school_MS         	sex_F             
sex_M             	age               	address_R         
address_U         	famsize_GT3       	famsize_LE3       
Pstatus_A         	Pstatus_T         	Medu              
Fedu              	Mjob_at_home      	Mjob_health       
Mjob_other        	Mjob_services     	Mjob_teacher      
Fjob_at_home      	Fjob_health       	Fjob_other        
Fjob_services     	Fjob_teacher      	reason_course     
reason_home       	reason_other      	reason_reputation 
guardian_father   	guardian_mother   	guardian_other    
traveltime        	studytime         	failures          
schoolsup         	famsup            	paid              
activities        	nursery           	higher            
internet          	romantic          	famrel            
freetime          	goout             	Dalc              
Walc              	health            	absences  
\end{lstlisting}

Now, all binary data will be converted into $0$/$1$ for future tests. The amount of features just reduced from 48 to 43.

% insert code snippet
\begin{lstlisting}
Processed feature columns (43):
===================

school            	sex               	age               
address           	famsize           	Pstatus           
Medu              	Fedu              	Mjob_at_home      
Mjob_health       	Mjob_other        	Mjob_services     
Mjob_teacher      	Fjob_at_home      	Fjob_health       
Fjob_other        	Fjob_services     	Fjob_teacher      
reason_course     	reason_home       	reason_other      
reason_reputation 	guardian_father   	guardian_mother   
guardian_other    	traveltime        	studytime         
failures          	schoolsup         	famsup            
paid              	activities        	nursery           
higher            	internet          	romantic          
famrel            	freetime          	goout             
Dalc              	Walc              	health            
absences    
\end{lstlisting}

\subsection{Split data into training and test sets}
So far, I have converted all \textit{categorical} features into numeric values. To be able to judge if the model chosen in the next section will generalize well from its experience, I will hold out part of the data to measure how the algorithms are performing on yet-unseen examples. In this next step, I am going to split the data (both features and corresponding labels) into training and test sets. I will use the function \textit{cross\_validation.train\_test\_split()} from \textit{scikit-learn} for that.

\begin{lstlisting}
Training set: 296 samples
Test set: 99 samples
\end{lstlisting}


\section{Training and Evaluating Models}
Choose 3 supervised learning models that are available in scikit-learn, and appropriate for this problem. For each model ...


\section{Choosing the Best Model}
...


\section{Reflection}
...



\bibliographystyle{plain}
\bibliography{bibliography/biblio.bib}

























\newpage
\section{Some LaTeX tips}
\label{sec:latex}
\subsection{How to Include Figures}

First you have to upload the image file (JPEG, PNG or PDF) from your computer to writeLaTeX using the upload link the project menu. Then use the includegraphics command to include it in your document. Use the figure environment and the caption command to add a number and a caption to your figure. See the code for Figure \ref{fig:frog} in this section for an example.

\begin{figure}[ht]
\centering
\includegraphics[width=0.3\textwidth]{frog.jpg}
\caption{\label{fig:frog}This frog was uploaded to writeLaTeX via the project menu.}
\end{figure}

\subsection{How to Make Tables}

Use the table and tabular commands for basic tables --- see Table~\ref{tab:widgets}, for example.

\begin{table}[ht]
\centering
\begin{tabular}{l|r}
Item & Quantity \\\hline
Widgets & 42 \\
Gadgets & 13
\end{tabular}
\caption{\label{tab:widgets}An example table.}
\end{table}

\subsection{How to Write Mathematics}

\LaTeX{} is great at typesetting mathematics. Let $X_1, X_2, \ldots, X_n$ be a sequence of independent and identically distributed random variables with $\text{E}[X_i] = \mu$ and $\text{Var}[X_i] = \sigma^2 < \infty$, and let

\begin{equation}
S_n = \frac{X_1 + X_2 + \cdots + X_n}{n}
      = \frac{1}{n}\sum_{i}^{n} X_i
\label{eq:sn}
\end{equation}

denote their mean. Then as $n$ approaches infinity, the random variables $\sqrt{n}(S_n - \mu)$ converge in distribution to a normal $\mathcal{N}(0, \sigma^2)$.

The equation \ref{eq:sn} is very nice.

\subsection{How to Make Sections and Subsections}

Use section and subsection commands to organize your document. \LaTeX{} handles all the formatting and numbering automatically. Use ref and label commands for cross-references.

\subsection{How to Make Lists}

You can make lists with automatic numbering \dots

\begin{enumerate}
\item Like this,
\item and like this.
\end{enumerate}
\dots or bullet points \dots
\begin{itemize}
\item Like this,
\item and like this.
\end{itemize}
\dots or with words and descriptions \dots
\begin{description}
\item[Word] Definition
\item[Concept] Explanation
\item[Idea] Text
\end{description}

We hope you find write\LaTeX\ useful, and please let us know if you have any feedback using the help menu above.


\begin{thebibliography}{9}
\bibitem{nano3}
  K. Grove-Rasmussen og Jesper Nyg√•rd,
  \emph{Kvantef√¶nomener i Nanosystemer}.
  Niels Bohr Institute \& Nano-Science Center, K√∏benhavns Universitet

\end{thebibliography}
\end{document}
